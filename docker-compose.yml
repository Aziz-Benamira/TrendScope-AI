version: '3.8'

services:
  # Zookeeper - Kafka dependency
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    volumes:
      - zookeeper_data:/var/lib/zookeeper/data
    networks:
      - trendscope-network

  # Kafka Broker
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    volumes:
      - kafka_data:/var/lib/kafka/data
    networks:
      - trendscope-network
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
      interval: 10s
      timeout: 10s
      retries: 5

  # Cassandra Database
  cassandra:
    image: cassandra:4.1
    container_name: cassandra
    ports:
      - "9042:9042"
    environment:
      CASSANDRA_CLUSTER_NAME: TrendScopeCluster
      CASSANDRA_DC: datacenter1
      CASSANDRA_ENDPOINT_SNITCH: GossipingPropertyFileSnitch
    volumes:
      - cassandra_data:/var/lib/cassandra
    networks:
      - trendscope-network
    healthcheck:
      test: ["CMD-SHELL", "cqlsh -e 'describe cluster'"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Spark Master
  spark-master:
    image: apache/spark:3.5.0
    container_name: spark-master
    command: >
      bash -c "
      pip install kafka-python==2.0.2 cassandra-driver==3.28.0 vaderSentiment==3.3.2 textblob==0.17.1 python-dotenv==1.0.0 colorlog==6.8.0 &&
      /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
      "
    environment:
      - SPARK_NO_DAEMONIZE=true
    ports:
      - "8080:8080"
      - "7077:7077"
    volumes:
      - ./processors:/opt/spark-apps
      - ./configs:/opt/spark-configs
      - ./.env:/opt/spark-apps/.env
    networks:
      - trendscope-network

  # Spark Worker
  spark-worker:
    image: apache/spark:3.5.0
    container_name: spark-worker
    depends_on:
      - spark-master
    command: >
      bash -c "
      pip install kafka-python==2.0.2 cassandra-driver==3.28.0 vaderSentiment==3.3.2 textblob==0.17.1 python-dotenv==1.0.0 colorlog==6.8.0 &&
      /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
      "
    environment:
      - SPARK_NO_DAEMONIZE=true
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    volumes:
      - ./processors:/opt/spark-apps
      - ./configs:/opt/spark-configs
    networks:
      - trendscope-network

  # MLflow Server
  mlflow:
    build:
      context: ./monitoring/mlflow
      dockerfile: Dockerfile
    container_name: mlflow
    ports:
      - "5000:5000"
    environment:
      - MLFLOW_BACKEND_STORE_URI=sqlite:///mlflow.db
      - MLFLOW_DEFAULT_ARTIFACT_ROOT=/mlflow/artifacts
    volumes:
      - mlflow_data:/mlflow
    networks:
      - trendscope-network
    command: mlflow server --host 0.0.0.0 --port 5000

  # Prometheus
  prometheus:
    image: prom/prometheus:v2.48.0
    container_name: prometheus
    ports:
      - "9091:9090"
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
    networks:
      - trendscope-network

  # Grafana
  grafana:
    image: grafana/grafana:10.2.2
    container_name: grafana
    ports:
      - "3001:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_INSTALL_PLUGINS=grafana-clock-panel
    volumes:
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards
      - grafana_data:/var/lib/grafana
    depends_on:
      - prometheus
    networks:
      - trendscope-network

  # TMDB Producer
  tmdb-producer:
    build:
      context: ./producers/tmdb
      dockerfile: Dockerfile
    container_name: tmdb-producer
    depends_on:
      kafka:
        condition: service_healthy
      data-loader:
        condition: service_completed_successfully
    environment:
      - TMDB_API_KEY=${TMDB_API_KEY}
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - KAFKA_TOPIC=${KAFKA_TOPIC_TMDB}
      - FETCH_INTERVAL=${TMDB_FETCH_INTERVAL}
    volumes:
      - ./producers/tmdb:/app
      - ./data_loader:/app/data_loader
      - ./tmdb_dataset_files:/data/imdb_datasets
    networks:
      - trendscope-network
    restart: unless-stopped

  # Reddit Producer
  reddit-producer:
    build:
      context: ./producers/reddit
      dockerfile: Dockerfile
    container_name: reddit-producer
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      - REDDIT_CLIENT_ID=${REDDIT_CLIENT_ID}
      - REDDIT_CLIENT_SECRET=${REDDIT_CLIENT_SECRET}
      - REDDIT_USER_AGENT=${REDDIT_USER_AGENT}
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - KAFKA_TOPIC=${KAFKA_TOPIC_REDDIT}
      - FETCH_INTERVAL=${REDDIT_FETCH_INTERVAL}
    volumes:
      - ./producers/reddit:/app
    networks:
      - trendscope-network
    restart: unless-stopped

  # Spark Streaming Processor
  spark-processor:
    image: apache/spark:3.5.0
    container_name: spark-processor
    user: root
    depends_on:
      - spark-master
      - kafka
      - cassandra
    command: >
      bash -c "
      pip install kafka-python==2.0.2 cassandra-driver==3.28.0 vaderSentiment==3.3.2 textblob==0.17.1 python-dotenv==1.0.0 colorlog==6.8.0 &&
      mkdir -p /home/spark/.ivy2/cache &&
      chown -R spark:spark /home/spark/.ivy2 &&
      chmod -R 755 /home/spark/.ivy2 &&
      sleep 20 &&
      su -c '/opt/spark/bin/spark-submit --master local[2] --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,com.datastax.spark:spark-cassandra-connector_2.12:3.4.1 --conf spark.cassandra.connection.host=cassandra --conf spark.cassandra.connection.port=9042 --conf spark.sql.streaming.checkpointLocation=/tmp/spark-checkpoint --conf spark.executor.memory=1g --conf spark.driver.memory=1g /opt/spark-apps/spark_streaming_processor.py' spark
      "
    environment:
      - SPARK_MASTER_URL=local[*]
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - CASSANDRA_HOST=cassandra
      - CASSANDRA_KEYSPACE=${CASSANDRA_KEYSPACE}
      - W1_POPULARITY=${W1_POPULARITY}
      - W2_MENTIONS=${W2_MENTIONS}
      - W3_SENTIMENT=${W3_SENTIMENT}
    volumes:
      - ./processors:/opt/spark-apps
      - ./.env:/opt/spark-apps/.env
    networks:
      - trendscope-network
    restart: unless-stopped

  # River ML Service
  ml-service:
    build:
      context: ./ml_service
      dockerfile: Dockerfile
    container_name: ml-service
    depends_on:
      - kafka
      - mlflow
      - cassandra
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - CASSANDRA_HOST=cassandra
      - CASSANDRA_KEYSPACE=${CASSANDRA_KEYSPACE}
    volumes:
      - ./ml_service:/app
    networks:
      - trendscope-network
    restart: unless-stopped

  # IMDb Data Loader
  data-loader:
    build:
      context: ./data_loader
      dockerfile: Dockerfile
    container_name: data-loader
    depends_on:
      - cassandra
    environment:
      - CASSANDRA_HOST=cassandra
      - CASSANDRA_KEYSPACE=imdb_features
    volumes:
      - ./data_loader:/app
      - ./tmdb_dataset_files:/data/imdb_datasets
    networks:
      - trendscope-network
    restart: "no"  # Run once and exit

  # FastAPI Backend
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: backend
    depends_on:
      - cassandra
      - kafka
      - chromadb
      - ollama
    ports:
      - "8001:8000"
    environment:
      - CASSANDRA_HOST=cassandra
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - CHROMA_HOST=chromadb
      - CHROMA_PORT=8000
      - OLLAMA_HOST=ollama
      - OLLAMA_PORT=11434
      - OLLAMA_MODEL=mistral
    volumes:
      - ./backend:/app
      - ./rag:/app/rag
    networks:
      - trendscope-network
    restart: unless-stopped

  # ═══════════════════════════════════════════════════════════════════════════
  # RAG LAYER (NEW)
  # ═══════════════════════════════════════════════════════════════════════════

  # ChromaDB - Vector Database for Review Embeddings
  chromadb:
    image: chromadb/chroma:latest
    container_name: chromadb
    ports:
      - "8000:8000"
    environment:
      - ANONYMIZED_TELEMETRY=FALSE
      - ALLOW_RESET=TRUE
    volumes:
      - chroma_data:/chroma/chroma
    networks:
      - trendscope-network
    restart: unless-stopped

  # Ollama - Local LLM Server (Mistral)
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - trendscope-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped

  # Embedding Service - Kafka to ChromaDB (Reddit comments)
  embedding-service:
    build:
      context: ./rag
      dockerfile: Dockerfile
    container_name: embedding-service
    depends_on:
      - kafka
      - chromadb
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - KAFKA_TOPIC_REDDIT=reddit_stream
      - CHROMA_HOST=chromadb
      - CHROMA_PORT=8000
      - EMBEDDING_MODEL=all-MiniLM-L6-v2
    volumes:
      - ./rag:/app
    networks:
      - trendscope-network
    restart: unless-stopped

  # Metadata Enrichment Service - TMDB to ChromaDB (Hybrid RAG)
  metadata-enrichment:
    build:
      context: ./rag
      dockerfile: Dockerfile
    container_name: metadata-enrichment
    depends_on:
      - kafka
      - chromadb
      - tmdb-producer
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - KAFKA_TOPIC_TMDB=tmdb_stream
      - CHROMA_HOST=chromadb
      - CHROMA_PORT=8000
      - EMBEDDING_MODEL=all-MiniLM-L6-v2
    command: python metadata_enrichment_service.py
    volumes:
      - ./rag:/app
    networks:
      - trendscope-network
    restart: unless-stopped

networks:
  trendscope-network:
    driver: bridge

volumes:
  zookeeper_data:
  kafka_data:
  cassandra_data:
  mlflow_data:
  prometheus_data:
  grafana_data:
  chroma_data:
  ollama_data:
